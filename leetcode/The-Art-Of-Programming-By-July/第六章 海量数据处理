1.关联式容器

2.分而治之

3.simhash算法
-背景
    如果某一天，面试官问你如何设计一个比较两篇文章相似度的算法？可能你会回答几个比较传统点的思路：
    一种方案是先将两篇文章分别进行分词，得到一系列特征向量，然后计算特征向量之间的距离（可以计算它们之间的欧氏距离、海明距离或者夹角余弦等等），从而通过距离的大小来判断两篇文章的相似度。
    另外一种方案是传统hash，我们考虑为每一个web文档通过hash的方式生成一个指纹（finger print）。

    下面，我们来分析下这两种方法。
    采取第一种方法，若是只比较两篇文章的相似性还好，但如果是海量数据呢，有着数以百万甚至亿万的网页，要求你计算这些网页的相似度。你还会去计算任意两个网页之间的距离或夹角余弦么？想必你不会了。
    而第二种方案中所说的传统加密方式md5，其设计的目的是为了让整个分布尽可能地均匀，但如果输入内容一旦出现哪怕轻微的变化，hash值就会发生很大的变化。

-出世
    simhash:专门用来解决亿万级别的网页的去重任务。
    其主要思想是降维，将高维的特征向量映射成低维的特征向量，通过两个向量的Hamming Distance来确定文章是否重复或者高度近似。
-流程
    simhash算法分为5个步骤：分词、hash、加权、合并、降维


4.外排序

5.MapReduce
    MapReduce是一种计算模型，简单的说就是将大批量的工作（数据）分解（MAP）执行，然后再将结果合并成最终结果（REDUCE）。这样做的好处是可以在任务被分解后，可以通过大量机器进行并行计算，减少整个操作的时间。但如果你要我再通俗点介绍，那么，说白了，Mapreduce的原理就是一个归并排序。
    适用范围：数据量大，但是数据种类小可以放入内存
    基本原理及要点：将数据交给不同的机器去处理，数据划分，结果归约。

    MapReduce是一种模式。
    Hadoop是一种框架。
    Hadoop是一个实现了MapReduce模式的开源的分布式并行编程框架。
    所以，你现在，知道了什么是MapReduce，什么是hadoop，以及这两者之间最简单的联系，而本文的主旨即是，一句话概括：在hadoop的框架上采取MapReduce的模式处理海量数据。


6.多层划分
-多层划分法，本质上还是分而治之的思想，因为元素范围很大，不能利用直接寻址表，所以通过多次划分，逐步确定范围，然后最后在一个可以接受的范围内进行。


7.bitmap-------容易产生冲突
-基本原理及要点:使用bit数组来表示某些元素是否存在，比如8位电话号码.


8.布隆过滤器-------相比bitmap,它采用k个哈希函数，从而降低了冲突的概率
-Bloom Filter是由Bloom在1970年提出的一种多哈希函数映射的快速查找算法。通常应用在一些需要快速判断某个元素是否属于集合，但是并不严格要求100%正确的场合。
-Bloom Filter算法如下：
    创建一个m位BitSet，先将所有位初始化为0，然后选择k个不同的哈希函数。第i个哈希函数对字符串str哈希的结果记为h（i，str），且h（i，str）的范围是0到m-1 。
-Java实现
    http://www.cnblogs.com/heaad/archive/2011/01/02/1924195.html


9.Trie树
-原理大概懂了 但不知道怎么实现？？？？


10.数据库


11.倒排索引
-在正向索引中，文档占据了中心的位置，每个文档指向了一个它所包含的索引项的序列。也就是说文档指向了它包含的那些单词，而反向索引则是单词指向了包含它的文档，很容易看到这个反向的关系。
-应用：搜索引擎、关键字查找

-倒排索引创建索引的流程：
    1） 首先把所有的原始数据进行编号，形成文档列表
    2） 把文档数据进行分词，得到很多的词条，以词条为索引。保存包含这些词条的文档的编号信息。
-搜索的过程：
    当用户输入任意的词条时，首先对用户输入的数据进行分词，得到用户要搜索的所有词条，
    然后拿着这些词条去倒排索引列表中进行匹配。
    找到这些词条就能找到包含这些词条的所有文档的编号。然后根据这些编号去文档列表中找到文档

    作者：水無刹那
    链接：https://www.zhihu.com/question/23202010/answer/254503794
    来源：知乎
    著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

12.Pearson 相关系数是用协方差除以两个变量的标准差得到的


